{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ce8a4f-64ec-4fad-b8f4-5654aef785f4",
   "metadata": {},
   "source": [
    "The user sends a query about Arize to your service.\n",
    "\n",
    "langchain.embeddings.OpenAIEmbeddings makes a request to OpenAI to embed the user query using the text-embedding-ada-002 model.\n",
    "\n",
    "We retrieve by searching against the entries of your Qdrant database for the most similar pieces of context by MMR.\n",
    "\n",
    "langchain.llms.ChatOpenAI generates a response by formatting the query and retrieved context into a single prompt and sending a request to OpenAI with the gpt-4-turbo-preview model.\n",
    "\n",
    "The response is returned to the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9289a84-decb-4a3b-9020-dee5e35a6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain qdrant-client langchain_community tiktoken cohere langchain-openai \"protobuf>=3.20.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc979bf-9de9-47e8-be84-9e0b94a889e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/uberdev/ddrv/telemetenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Third-party library imports\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Phoenix imports\n",
    "import phoenix as px\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import GitbookLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import ChatOpenAI\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "# Miscellaneous imports\n",
    "\n",
    "# Configuration and Initialization\n",
    "nest_asyncio.apply()\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50371691-7bdc-450b-b27b-a73d68bae12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"/media/uberdev/ddrv/gitFolders/python_de_learners_data/.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd35058-3165-432e-827e-b09272e8df04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/uberdev/ddrv/telemetenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:141: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "model_embed = \"text-embedding-ada-002\"\n",
    "embeddings = OpenAIEmbeddings(model=model_embed, \n",
    "                              api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9601592d-09cc-40fa-96a8-9aaf1a9df6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/html/parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "Fetching pages: 100%|##########| 260/260 [02:59<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_gitbook_docs(docs_url):\n",
    "    \"\"\"\n",
    "    Loads documentation from a Gitbook URL.\n",
    "    \"\"\"\n",
    "\n",
    "    loader = GitbookLoader(\n",
    "        docs_url,\n",
    "        load_all_paths=True,\n",
    "    )\n",
    "    return loader.load()\n",
    "\n",
    "\n",
    "docs_url = \"https://docs.arize.com/arize/\"\n",
    "docs = load_gitbook_docs(docs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ab6425-6669-4a5c-ba7c-58c38fa14dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(docs)\n",
    "len(docs[0].page_content.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ea730e-2e4d-4129-a83a-e005579655e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "# embedding is going to happen in this area...\n",
    "# take care of the documents you are sending \n",
    "# through the OpenAI embedding model\n",
    "qdrant = Qdrant.from_documents(\n",
    "    docs[:10], # only embed 10 documents\n",
    "    embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=\"arize_docs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "815cda51-ff97-46ba-9240-2c359a371515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:phoenix.trace.exporter:Arize Phoenix is not running on http://127.0.0.1:6006/. Launch Phoenix with `import phoenix as px; px.launch_app()`\n"
     ]
    }
   ],
   "source": [
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9a2a6f-d706-44db-abc1-d78731b81628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the langchain appln\n",
    "\n",
    "handler = StdOutCallbackHandler()\n",
    "\n",
    "num_retrieved_documents = 2\n",
    "\n",
    "retriever = qdrant.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": num_retrieved_documents}, enable_limit=True\n",
    ")\n",
    "\n",
    "chain_type = \"stuff\"  # stuff, refine, map_reduce, and map_rerank\n",
    "\n",
    "chat_model_name = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b4b9eba-bc32-4ca3-a2ce-35da9e3c7c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=chat_model_name,\n",
    "                 temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "691dba14-5706-49d2-b76f-dd2ea535d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=chain_type,\n",
    "    retriever=retriever,\n",
    "    metadata={\"application_type\": \"question_answering\"},\n",
    "    callbacks=[handler],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fef86409-e757-4907-a58e-65dc52193d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.read_parquet(\n",
    "    \"http://storage.googleapis.com/arize-phoenix-assets/datasets/unstructured/llm/context-retrieval/langchain/langchain_query_dataframe_with_user_feedbackv2.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd65bc6-7a72-4aa7-855c-3c722a574a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:phoenix.config:📋 Ensuring phoenix working directory: /home/uberdev/.phoenix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n",
      "📺 Opening a view to the Phoenix app. The app is running at http://localhost:6006/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"1000\"\n",
       "            src=\"http://localhost:6006/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7e116d5ab5e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(session := px.launch_app()).view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad8768d5-7fad-4b91-98c5-277b3ce11e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'How do I use the SDK to upload a ranking model?', 'result': \"I don't know.\"}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'What drift metrics are supported in Arize?', 'result': \"I don't know.\"}\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    row = query_df.iloc[i]\n",
    "    response = chain.invoke(row[\"text\"])\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dab2f458-d53b-435d-8578-6b1f7068d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.evals import (\n",
    "    RAG_RELEVANCY_PROMPT_RAILS_MAP,\n",
    "    RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    OpenAIModel,\n",
    "    llm_classify,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7217905-a54d-4a35-8dad-6a9f1264af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluation dataframes with \"input\" and \"reference\" columns\n",
    "context0_eval_df = query_df.copy()\n",
    "context0_eval_df[\"input\"] = context0_eval_df[\"text\"]\n",
    "context0_eval_df[\"reference\"] = context0_eval_df[\"context_text_0\"]\n",
    "\n",
    "context1_eval_df = query_df.copy()\n",
    "context1_eval_df[\"input\"] = context1_eval_df[\"text\"]\n",
    "context1_eval_df[\"reference\"] = context1_eval_df[\"context_text_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ede22bf9-577e-4a0b-9c2d-5aa9f2065ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(True, 'relevant'), (False, 'unrelated')])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_RELEVANCY_PROMPT_RAILS_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "942e52bb-e61e-4a38-8798-4f5499914445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "You are comparing a reference text to a question and trying to determine if the reference text\n",
       "contains information relevant to answering the question. Here is the data:\n",
       "    [BEGIN DATA]\n",
       "    ************\n",
       "    [Question]: {input}\n",
       "    ************\n",
       "    [Reference text]: {reference}\n",
       "    ************\n",
       "    [END DATA]\n",
       "Compare the Question above to the Reference text. You must determine whether the Reference text\n",
       "contains information that can answer the Question. Please focus on whether the very specific\n",
       "question can be answered by the information in the Reference text.\n",
       "Your response must be single word, either \"relevant\" or \"unrelated\",\n",
       "and should not contain any text or characters aside from that word.\n",
       "\"unrelated\" means that the reference text does not contain an answer to the Question.\n",
       "\"relevant\" means the reference text contains an answer to the Question."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAG_RELEVANCY_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a114dda7-47c5-459e-a428-8137f486dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(model=\"gpt-4o-mini\")\n",
    "#  A novel idea is to use LLMs to evaluate retrieval quality \n",
    "# by simply asking the LLM whether each piece of retrieved\n",
    "# context is relevant or irrelevant to the corresponding \n",
    "# query.\n",
    "\n",
    "context0_relevance = llm_classify(\n",
    "    context0_eval_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    "    model=model,\n",
    ")\n",
    "context1_relevance = llm_classify(\n",
    "    context1_eval_df,\n",
    "    template=RAG_RELEVANCY_PROMPT_TEMPLATE,\n",
    "    rails=list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values()),\n",
    "    provide_explanation=True,\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ab9c3-7694-4362-9782-6ca1ed2874d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query_df = query_df.copy()\n",
    "sample_query_df[\"openai_relevance_0\"] = context0_relevance[\"label\"]\n",
    "sample_query_df[\"openai_relevance_1\"] = context1_relevance[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d7c53-eeb5-4da2-9f17-7fd5823829b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_relevant_documents_array = np.zeros(len(sample_query_df))\n",
    "num_retrieved_documents = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17896819-0e11-43bd-a0d5-5e29c6992289",
   "metadata": {},
   "outputs": [],
   "source": [
    "for retrieved_document_index in range(0, num_retrieved_documents):\n",
    "    num_retrieved_documents = retrieved_document_index + 1\n",
    "    num_relevant_documents_array += (\n",
    "        sample_query_df[f\"openai_relevance_{retrieved_document_index}\"]\n",
    "        .map(lambda x: int(x == \"relevant\"))\n",
    "        .to_numpy()\n",
    "    )\n",
    "    sample_query_df[f\"openai_precision@{num_retrieved_documents}\"] = pd.Series(\n",
    "        num_relevant_documents_array / num_retrieved_documents\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb040b-565d-425e-bfa6-f694a82a41b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_query_df[\n",
    "    [\n",
    "        \"openai_relevance_0\",\n",
    "        \"openai_relevance_1\",\n",
    "        \"openai_precision@1\",\n",
    "        \"openai_precision@2\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14285920-1869-427a-b7d4-8f9e2644652c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
